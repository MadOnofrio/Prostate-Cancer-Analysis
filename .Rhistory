attach(credit.rating)
options(scipen = 999)
str(credit.rating)
credit.rating <- credit.rating[2:8]
names(credit.rating)[1] <- "WC_TA"
names(credit.rating)[2] <- "RE_TA"
names(credit.rating)[3] <- "EBIT_TA"
names(credit.rating)[4] <- "MVE_BVTD"
names(credit.rating)[5] <- "S_TA"
names(credit.rating)[6] <- "ISL"
names(credit.rating)[7] <- "RATING"
summary(credit.rating[-c(7)])
par(mfrow=c(2,3))
hist(credit.rating$WC_TA, main = "WC/TA",col = "green", xlim = c(-1,1))
hist(credit.rating$RE_TA, main = "RE/TA",col = "red", xlim = c(-1.5,1), ylim = c(0,3000))
hist(credit.rating$EBIT_TA, main = "EBIT/TA",col = "blue", xlim = c(-0.2,0.3), ylim = c(0,2000))
hist(credit.rating$MVE_BVTD, main = "MVE/BVTD",col = "orange", xlim = c(0,25), ylim = c(0,5000))
hist(credit.rating$S_TA, main = "S/TA",col = "purple", xlim = c(0,2))
hist(credit.rating$ISL, main = "ISL",col = "yellow")
library(car)
scatterplotMatrix( ~ WC_TA+RE_TA+EBIT_TA+MVE_BVTD+S_TA+ISL,
col="black", pch=20, regLine = list(method=lm, lty=1, lwd=2, col="chartreuse3"),
smooth=FALSE,diagonal=list(method="density",bw="nrd0",adjust=1, kernel="gaussian",na.rm=TRUE), main="Matrice di dispersione con rette di regressione", data=credit.rating)
credit.rating$ISL <- as.factor(credit.rating$ISL)
library(ggplot2)
library(hrbrthemes)
ggplot(credit.rating, aes(x=WC_TA, y=RE_TA, color=RATING,)) +
geom_point(size=2) +
theme_ipsum() +
xlab("WC/TA") + ylab("RE/TA") +
scale_color_manual(values=c(1:7))
credit.rating.fit <- credit.rating[-c(6,7)]
detach(credit.rating)
attach(credit.rating.fit)
mat.cor <- cor(credit.rating.fit)
library(corrplot)
corrplot(mat.cor, method = "color", type = "upper",number.cex = .8, order="hclust", addCoef.col = "black",tl.col = "#CC9900", diag=F)
wc_ta.isl <- sqrt(summary(lm(WC_TA ~ ISL,data = credit.rating))$r.squared)
re_ta.isl <- sqrt(summary(lm(RE_TA ~ ISL,data = credit.rating))$r.squared)
ebit_ta.isl <- sqrt(summary(lm(EBIT_TA ~ ISL,data = credit.rating))$r.squared)
mve_bvtd.isl <- sqrt(summary(lm(MVE_BVTD ~ ISL,data = credit.rating))$r.squared)
s_ta.isl <- sqrt(summary(lm(S_TA ~ ISL,data = credit.rating))$r.squared)
mat.cor.factor <- data.frame(wc_ta.isl,re_ta.isl,ebit_ta.isl,mve_bvtd.isl,s_ta.isl)
knitr::kable(mat.cor.factor, "simple")
pr.out <- prcomp(credit.rating.fit,scale=T, center=T)
summary(pr.out)
names(pr.out)
dim(pr.out$x)
pr.out$sdev
dim(pr.out$rotation)
pr.var <- (pr.out$sdev)^2
pve <- pr.var/sum(pr.var)
pve.perc <- pve*100
pve.perc
par(mfrow = c(1,2))
plot(pve,xlab = "PCA",ylab = "Porzione di variabilità siegata",ylim = c(0,1),type = "b", main = "Scree Plot")
plot(cumsum(pve),xlab = "PCA",ylab = "Porzione di variabilità siegata cumulata",ylim = c(0,1),type = "b", main  = "Scree Plot")
knitr::kable(abs(pr.out$rotation[,1:2]), "simple")
mat.corPCA <- pr.out$rotation
corrplot(mat.corPCA, method = "color",number.cex = .8, order = "hclust", addCoef.col = "black",tl.col = "#CC9900")
#biplot(pr.out,choices = 1:2, col = c('black','red'))
#abline(v = 0, col = "blue")
#abline(h = 0, col = "blue")
knitr::include_graphics("biplot.jpeg")
prostate.data <- read.delim("prostate.csv")
prostate <- prostate.data[-c(1,11)]
#eliminazione della prima e undicesima variabile
attach(prostate)
str(prostate)
prostate$svi <- as.factor(prostate$svi)
summary(prostate[-c(5)])
par(mfrow=c(2,2))
boxplot(prostate$lcavol, col = "blue", main = "Volume")
boxplot(prostate$lweight, col = "red", main = "Peso")
boxplot(prostate$lcp, col = "yellow", main = "Penetrazione capsulare")
boxplot(prostate$lpsa, col = "green", main = "Antigene prostatico specifico")
par(mfrow=c(3,3), mar=c(4,4,2,0.5))
for (j in 1:ncol(prostate[-c(5)])) {
hist(prostate[-c(5)][,j], xlab=colnames(prostate[-c(5)])[j],
main=paste("Histogram of", colnames(prostate[-c(5)])[j]),
col="lightblue", breaks=20)
}
mat.cor <- cor(prostate[-c(5)])
library(corrplot)
corrplot(mat.cor, method = "color", type = "upper",number.cex = .8, order="hclust", addCoef.col = "black",tl.col = "#CC9900", diag=F)
lcavol.svi <- sqrt(summary(lm(lcavol ~ svi,data = prostate))$r.squared)
lweight.svi <- sqrt(summary(lm(lweight ~ svi,data = prostate))$r.squared)
age.svi <- sqrt(summary(lm(age ~ svi,data = prostate))$r.squared)
lbph.svi <- sqrt(summary(lm(lbph ~ svi,data = prostate))$r.squared)
lcp.svi <- sqrt(summary(lm(lcp ~ svi,data = prostate))$r.squared)
pgg45.svi <- sqrt(summary(lm(pgg45 ~ svi,data = prostate))$r.squared)
lpsa.svi <- sqrt(summary(lm(lpsa ~ svi,data = prostate))$r.squared)
mat.cor.factor <- data.frame(lcavol.svi,lweight.svi,age.svi,lbph.svi,lbph.svi,lcp.svi,pgg45.svi,lpsa.svi)
knitr::kable(mat.cor.factor, "simple")
prostate.fit <- prostate[-c(6,8)]
detach(prostate)
attach(prostate.fit)
library(car)
scatterplotMatrix( ~ lcavol+lweight+age+lbph+lpsa, col="black", pch=20, regLine = list(method=lm, lty=1, lwd=2, col="red"),smooth=F,diagonal=list(method="density",bw="nrd0",adjust=1, kernel="gaussian",na.rm=T), main="Matrice di dispersione con rette di regressione", data=prostate)
summary(lm(lpsa~.,data=prostate.fit))
prostate.train <- subset(prostate.fit,prostate.data$train==T)
prostate.train <- prostate.train[-c(10)]
prostate.test <-  subset(prostate.fit,prostate.data$train==F)
prostate.test <- prostate.test[-c(10)]
train <- prostate.train[-c(9)]
test <- prostate.test[-c(9)]
library(dplyr)
x.train <- model.matrix(prostate.train$lpsa~.,train)
x.test <- model.matrix(prostate.test$lpsa~., test)
y.train <- prostate.train %>%
select(lpsa) %>%
unlist() %>%
as.numeric()
y.test <- prostate.test %>%
select(lpsa) %>%
unlist() %>%
as.numeric()
ols.null <- lm(lpsa ~ 1,data=prostate.train)
ols.1 <- lm(lpsa ~ lcavol, data=prostate.train)
ols.2 <- lm(lpsa ~ lcavol + lweight, data=prostate.train)
ols.3 <- lm(lpsa ~ lcavol + lweight + age, data=prostate.train)
ols.4 <-lm(lpsa ~ lcavol + lweight + age + lbph, data=prostate.train)
ols.5 <- lm(lpsa ~ lcavol + lweight + age + lbph + svi, data=prostate.train)
ols.all <- lm(lpsa ~.,data=prostate.train)
bic.ols.null <- BIC(ols.null)
bic.ols.1 <- BIC(ols.1)
bic.ols.2 <- BIC(ols.2)
bic.ols.3 <- BIC(ols.3)
bic.ols.4 <- BIC(ols.4)
bic.ols.5 <- BIC(ols.5)
bic.ols.all <- BIC(ols.all)
bics <- cbind (bic.ols.null,bic.ols.1,bic.ols.2,bic.ols.3,bic.ols.4,bic.ols.5,bic.ols.all)
colnames(bics) <- c("OLS NULL","OLS 1","OLS 2","OLS 3","OLS 4","OLS 5","OLS ALL")
knitr::kable(t(bics), "simple")
summary(ols.2)
summary(ols.all)
predict.ols.2 <- predict(ols.2, newx = x.test)
mean((predict.ols.2 - y.test)^2) #MSE
library(glmnet)
grid <- 10^seq(10, -2, length = 100)
ridge.mod <- glmnet(x.train, y.train, alpha = 0, lambda = grid, thresh = 1e-12)
par(mfrow=c(1,2))
plot(ridge.mod, xvar = "lambda", lwd = 2, col=c(1:7), xlim = c(-5,12))
abline(h=0,col="black", lty=2)
legend("topright", legend=names(prostate.fit), fill = c(1:7), cex=0.8)
plot(ridge.mod, lwd = 2, col=c(1:7))
abline(h=0,col="black", lty=2)
ridge.pred <- predict(ridge.mod, s = 4, newx = x.test)
mean((ridge.pred - y.test)^2) #MSE
set.seed(1)
#Fittiamo la regressione ridge sul training set
cv.out.ridge <- cv.glmnet(x.train, y.train, alpha = 0)
plot(cv.out.ridge)
#Selezioniamo il lambda che ci restituisce il minor MSE
bestlam.ridge <- cv.out.ridge$lambda.min
bestlam.ridge
#Usiamo il miglior lambda per effettuare una previsione sul test set
ridge.pred <- predict(ridge.mod, s = bestlam.ridge, newx = x.test)
mean((ridge.pred - y.test)^2) #MSE test
tLL <- ridge.mod$nulldev - deviance(ridge.mod)
k <- ridge.mod$df
n <- ridge.mod$nobs
BIC.ridge<-log(n)*k - tLL
best.bic.ridge <- which.min(BIC.ridge)
best.bic.ridge #Posizione miglior BIC
plot(BIC.ridge,cex=2)
points(best.bic.ridge,BIC.ridge[best.bic.ridge],pch=19,cex=2)
BIC.ridge[best.bic.ridge] #Valore miglior BIC
#Fit del modello LASSO sul training
lasso.mod <- glmnet(x.train, y.train, alpha = 1, lambda = grid)
par(mfrow=c(1,2))
plot(lasso.mod, lwd = 2, col=c(1:7), xvar = "lambda", xlim = c(-5,5))
abline(h=0,col="black", lty=2)
legend("topright", legend=names(ols.all$coefficients[-c(1)]), fill = c(1:7), cex=0.8)
plot(lasso.mod, lwd = 2, col=c(1:7))
abline(h=0,col="black", lty=2)
set.seed(1)
cv.out.lasso <- cv.glmnet(x.train, y.train, alpha = 1) #Fit del modello LASSO sul training
plot(cv.out.lasso) #Mostriamo il grafico degli MSE di training in funzione di lambda
#Selezionamo il lambda che minimizza l'MSE del training
bestlam.lasso <- cv.out.lasso$lambda.min
bestlam.lasso
#Utilizziamo il miglior lambda per la nostra previsione
lasso.pred <- predict(lasso.mod, s = bestlam.lasso, newx = x.test)
mean((lasso.pred - y.test)^2) #MSE
tLL <- lasso.mod$nulldev - deviance(lasso.mod)
k <- lasso.mod$df
n <- lasso.mod$nobs
BIC.lasso <- log(n)*k - tLL
best.bic.lasso <- which.min(BIC.lasso)
best.bic.lasso #Posizione miglior BIC
plot(BIC.lasso,cex=2)
points(best.bic.lasso,BIC.lasso[best.bic.lasso],pch=19,cex=2)
BIC.lasso[best.bic.lasso] #Valore miglior BIC
x <- model.matrix(lpsa~., prostate.fit)[,-1] # elminazione dell'intercetta
y <- prostate.data %>%
select(lpsa) %>%
unlist() %>%
as.numeric()
out.lasso <- glmnet(x, y, alpha = 1, lambda = grid)
#Fittiamo la Lasso sul dataset completo
lasso.coef <- predict(out.lasso, type = "coefficients", s = bestlam.lasso)
#Mostriamo i soli coefficienti non nulli usando il lambda risultante dalla CV
lasso.coef[lasso.coef != 0]
out.ridge <- glmnet(x, y, alpha = 0, lambda = grid)
#Fittiamo la Ridge sul dataset completo
ridge.coef <- predict(out.ridge, type = "coefficients", s = bestlam.ridge)
#Mostriamo i soli coefficienti non nulli usando il lambda risultante dalla CV
ridge.coef[ridge.coef != 0]
levels(svi)
par(mfrow=c(1,2))
plot(ridge.mod, xvar = "lambda", lwd = 2, col=c(1:7), xlim = c(-5,12))
abline(h=0,col="black", lty=2)
legend("topright", legend=names(prostate.fit), fill = c(1:7), cex=0.8)
plot(ridge.mod, lwd = 2, col=c(1:7))
abline(h=0,col="black", lty=2)
par(mfrow=c(1,2))
plot(lasso.mod, lwd = 2, col=c(1:7), xvar = "lambda", xlim = c(-5,5))
abline(h=0,col="black", lty=2)
legend("topright", legend=names(ols.all$coefficients[-c(1)]), fill = c(1:7), cex=0.8)
plot(lasso.mod, lwd = 2, col=c(1:7))
abline(h=0,col="black", lty=2)
prostate.data <- read.delim("prostate.csv")
prostate <- prostate.data[-c(1,11)]
#eliminazione della prima e undicesima variabile
attach(prostate)
str(prostate)
prostate$svi <- as.factor(prostate$svi)
summary(prostate[-c(5)])
par(mfrow=c(2,2))
boxplot(prostate$lcavol, col = "blue", main = "Volume")
boxplot(prostate$lweight, col = "red", main = "Peso")
boxplot(prostate$lcp, col = "yellow", main = "Penetrazione capsulare")
boxplot(prostate$lpsa, col = "green", main = "Antigene prostatico specifico")
par(mfrow=c(3,3), mar=c(4,4,2,0.5))
for (j in 1:ncol(prostate[-c(5)])) {
hist(prostate[-c(5)][,j], xlab=colnames(prostate[-c(5)])[j],
main=paste("Histogram of", colnames(prostate[-c(5)])[j]),
col="lightblue", breaks=20)
}
mat.cor <- cor(prostate[-c(5)])
library(corrplot)
corrplot(mat.cor, method = "color", type = "upper",number.cex = .8, order="hclust", addCoef.col = "black",tl.col = "#CC9900", diag=F)
lcavol.svi <- sqrt(summary(lm(lcavol ~ svi,data = prostate))$r.squared)
lweight.svi <- sqrt(summary(lm(lweight ~ svi,data = prostate))$r.squared)
age.svi <- sqrt(summary(lm(age ~ svi,data = prostate))$r.squared)
lbph.svi <- sqrt(summary(lm(lbph ~ svi,data = prostate))$r.squared)
lcp.svi <- sqrt(summary(lm(lcp ~ svi,data = prostate))$r.squared)
pgg45.svi <- sqrt(summary(lm(pgg45 ~ svi,data = prostate))$r.squared)
lpsa.svi <- sqrt(summary(lm(lpsa ~ svi,data = prostate))$r.squared)
mat.cor.factor <- data.frame(lcavol.svi,lweight.svi,age.svi,lbph.svi,lbph.svi,lcp.svi,pgg45.svi,lpsa.svi)
knitr::kable(mat.cor.factor, "simple")
prostate.fit <- prostate[-c(6,8)]
detach(prostate)
attach(prostate.fit)
library(car)
scatterplotMatrix( ~ lcavol+lweight+age+lbph+lpsa, col="black", pch=20, regLine = list(method=lm, lty=1, lwd=2, col="red"),smooth=F,diagonal=list(method="density",bw="nrd0",adjust=1, kernel="gaussian",na.rm=T), main="Matrice di dispersione con rette di regressione", data=prostate)
summary(lm(lpsa~.,data=prostate.fit))
prostate.train <- subset(prostate.fit,prostate.data$train==T)
prostate.train <- prostate.train[-c(10)]
prostate.test <-  subset(prostate.fit,prostate.data$train==F)
prostate.test <- prostate.test[-c(10)]
train <- prostate.train[-c(9)]
test <- prostate.test[-c(9)]
library(dplyr)
x.train <- model.matrix(prostate.train$lpsa~.,train)
x.test <- model.matrix(prostate.test$lpsa~., test)
y.train <- prostate.train %>%
select(lpsa) %>%
unlist() %>%
as.numeric()
y.test <- prostate.test %>%
select(lpsa) %>%
unlist() %>%
as.numeric()
ols.null <- lm(lpsa ~ 1,data=prostate.train)
ols.1 <- lm(lpsa ~ lcavol, data=prostate.train)
ols.2 <- lm(lpsa ~ lcavol + lweight, data=prostate.train)
ols.3 <- lm(lpsa ~ lcavol + lweight + age, data=prostate.train)
ols.4 <-lm(lpsa ~ lcavol + lweight + age + lbph, data=prostate.train)
ols.5 <- lm(lpsa ~ lcavol + lweight + age + lbph + svi, data=prostate.train)
ols.all <- lm(lpsa ~.,data=prostate.train)
bic.ols.null <- BIC(ols.null)
bic.ols.1 <- BIC(ols.1)
bic.ols.2 <- BIC(ols.2)
bic.ols.3 <- BIC(ols.3)
bic.ols.4 <- BIC(ols.4)
bic.ols.5 <- BIC(ols.5)
bic.ols.all <- BIC(ols.all)
bics <- cbind (bic.ols.null,bic.ols.1,bic.ols.2,bic.ols.3,bic.ols.4,bic.ols.5,bic.ols.all)
colnames(bics) <- c("OLS NULL","OLS 1","OLS 2","OLS 3","OLS 4","OLS 5","OLS ALL")
knitr::kable(t(bics), "simple")
summary(ols.2)
summary(ols.all)
predict.ols.2 <- predict(ols.2, newx = x.test)
mean((predict.ols.2 - y.test)^2) #MSE
library(glmnet)
grid <- 10^seq(10, -2, length = 100)
ridge.mod <- glmnet(x.train, y.train, alpha = 0, lambda = grid, thresh = 1e-12)
par(mfrow=c(1,2))
plot(ridge.mod, xvar = "lambda", lwd = 2, col=c(1:7), xlim = c(-5,12))
abline(h=0,col="black", lty=2)
legend("topright", legend=names(prostate.fit), fill = c(1:7), cex=0.8)
plot(ridge.mod, lwd = 2, col=c(1:7))
abline(h=0,col="black", lty=2)
ridge.pred <- predict(ridge.mod, s = 4, newx = x.test)
mean((ridge.pred - y.test)^2) #MSE
set.seed(1)
#Fittiamo la regressione ridge sul training set
cv.out.ridge <- cv.glmnet(x.train, y.train, alpha = 0)
plot(cv.out.ridge)
#Selezioniamo il lambda che ci restituisce il minor MSE
bestlam.ridge <- cv.out.ridge$lambda.min
bestlam.ridge
#Usiamo il miglior lambda per effettuare una previsione sul test set
ridge.pred <- predict(ridge.mod, s = bestlam.ridge, newx = x.test)
mean((ridge.pred - y.test)^2) #MSE test
tLL <- ridge.mod$nulldev - deviance(ridge.mod)
k <- ridge.mod$df
n <- ridge.mod$nobs
BIC.ridge<-log(n)*k - tLL
best.bic.ridge <- which.min(BIC.ridge)
best.bic.ridge #Posizione miglior BIC
plot(BIC.ridge,cex=2)
points(best.bic.ridge,BIC.ridge[best.bic.ridge],pch=19,cex=2)
BIC.ridge[best.bic.ridge] #Valore miglior BIC
#Fit del modello LASSO sul training
lasso.mod <- glmnet(x.train, y.train, alpha = 1, lambda = grid)
par(mfrow=c(1,2))
plot(lasso.mod, lwd = 2, col=c(1:7), xvar = "lambda", xlim = c(-5,5))
abline(h=0,col="black", lty=2)
legend("topright", legend=names(ols.all$coefficients[-c(1)]), fill = c(1:7), cex=0.8)
plot(lasso.mod, lwd = 2, col=c(1:7))
abline(h=0,col="black", lty=2)
set.seed(1)
cv.out.lasso <- cv.glmnet(x.train, y.train, alpha = 1) #Fit del modello LASSO sul training
plot(cv.out.lasso) #Mostriamo il grafico degli MSE di training in funzione di lambda
#Selezionamo il lambda che minimizza l'MSE del training
bestlam.lasso <- cv.out.lasso$lambda.min
bestlam.lasso
#Utilizziamo il miglior lambda per la nostra previsione
lasso.pred <- predict(lasso.mod, s = bestlam.lasso, newx = x.test)
mean((lasso.pred - y.test)^2) #MSE
tLL <- lasso.mod$nulldev - deviance(lasso.mod)
k <- lasso.mod$df
n <- lasso.mod$nobs
BIC.lasso <- log(n)*k - tLL
best.bic.lasso <- which.min(BIC.lasso)
best.bic.lasso #Posizione miglior BIC
plot(BIC.lasso,cex=2)
points(best.bic.lasso,BIC.lasso[best.bic.lasso],pch=19,cex=2)
BIC.lasso[best.bic.lasso] #Valore miglior BIC
x <- model.matrix(lpsa~., prostate.fit)[,-1] # elminazione dell'intercetta
y <- prostate.data %>%
select(lpsa) %>%
unlist() %>%
as.numeric()
out.lasso <- glmnet(x, y, alpha = 1, lambda = grid)
#Fittiamo la Lasso sul dataset completo
lasso.coef <- predict(out.lasso, type = "coefficients", s = bestlam.lasso)
#Mostriamo i soli coefficienti non nulli usando il lambda risultante dalla CV
lasso.coef[lasso.coef != 0]
out.ridge <- glmnet(x, y, alpha = 0, lambda = grid)
#Fittiamo la Ridge sul dataset completo
ridge.coef <- predict(out.ridge, type = "coefficients", s = bestlam.ridge)
#Mostriamo i soli coefficienti non nulli usando il lambda risultante dalla CV
ridge.coef[ridge.coef != 0]
names(prostate)
names(prostate.data)
names(prostate.fit)
names(prostate.test)
names(prostate.train)
names(x.train)
prostate.data <- read.delim("prostate.csv")
prostate <- prostate.data[-c(1,11)]
#eliminazione della prima e undicesima variabile
attach(prostate)
str(prostate)
prostate$svi <- as.factor(prostate$svi)
summary(prostate[-c(5)])
par(mfrow=c(2,2))
boxplot(prostate$lcavol, col = "blue", main = "Volume")
boxplot(prostate$lweight, col = "red", main = "Peso")
boxplot(prostate$lcp, col = "yellow", main = "Penetrazione capsulare")
boxplot(prostate$lpsa, col = "green", main = "Antigene prostatico specifico")
par(mfrow=c(3,3), mar=c(4,4,2,0.5))
for (j in 1:ncol(prostate[-c(5)])) {
hist(prostate[-c(5)][,j], xlab=colnames(prostate[-c(5)])[j],
main=paste("Histogram of", colnames(prostate[-c(5)])[j]),
col="lightblue", breaks=20)
}
mat.cor <- cor(prostate[-c(5)])
library(corrplot)
corrplot(mat.cor, method = "color", type = "upper",number.cex = .8, order="hclust", addCoef.col = "black",tl.col = "#CC9900", diag=F)
lcavol.svi <- sqrt(summary(lm(lcavol ~ svi,data = prostate))$r.squared)
lweight.svi <- sqrt(summary(lm(lweight ~ svi,data = prostate))$r.squared)
age.svi <- sqrt(summary(lm(age ~ svi,data = prostate))$r.squared)
lbph.svi <- sqrt(summary(lm(lbph ~ svi,data = prostate))$r.squared)
lcp.svi <- sqrt(summary(lm(lcp ~ svi,data = prostate))$r.squared)
pgg45.svi <- sqrt(summary(lm(pgg45 ~ svi,data = prostate))$r.squared)
lpsa.svi <- sqrt(summary(lm(lpsa ~ svi,data = prostate))$r.squared)
mat.cor.factor <- data.frame(lcavol.svi,lweight.svi,age.svi,lbph.svi,lbph.svi,lcp.svi,pgg45.svi,lpsa.svi)
knitr::kable(mat.cor.factor, "simple")
prostate.fit <- prostate[-c(6,8)]
detach(prostate)
attach(prostate.fit)
library(car)
scatterplotMatrix( ~ lcavol+lweight+age+lbph+lpsa, col="black", pch=20, regLine = list(method=lm, lty=1, lwd=2, col="red"),smooth=F,diagonal=list(method="density",bw="nrd0",adjust=1, kernel="gaussian",na.rm=T), main="Matrice di dispersione con rette di regressione", data=prostate)
summary(lm(lpsa~.,data=prostate.fit))
prostate.train <- subset(prostate.fit,prostate.data$train==T)
prostate.train <- prostate.train[-c(10)]
prostate.test <-  subset(prostate.fit,prostate.data$train==F)
prostate.test <- prostate.test[-c(10)]
train <- prostate.train[-c(9)]
test <- prostate.test[-c(9)]
library(dplyr)
x.train <- model.matrix(prostate.train$lpsa~.,train)
x.test <- model.matrix(prostate.test$lpsa~., test)
y.train <- prostate.train %>%
select(lpsa) %>%
unlist() %>%
as.numeric()
y.test <- prostate.test %>%
select(lpsa) %>%
unlist() %>%
as.numeric()
ols.null <- lm(lpsa ~ 1,data=prostate.train)
ols.1 <- lm(lpsa ~ lcavol, data=prostate.train)
ols.2 <- lm(lpsa ~ lcavol + lweight, data=prostate.train)
ols.3 <- lm(lpsa ~ lcavol + lweight + age, data=prostate.train)
ols.4 <-lm(lpsa ~ lcavol + lweight + age + lbph, data=prostate.train)
ols.5 <- lm(lpsa ~ lcavol + lweight + age + lbph + svi, data=prostate.train)
ols.all <- lm(lpsa ~.,data=prostate.train)
bic.ols.null <- BIC(ols.null)
bic.ols.1 <- BIC(ols.1)
bic.ols.2 <- BIC(ols.2)
bic.ols.3 <- BIC(ols.3)
bic.ols.4 <- BIC(ols.4)
bic.ols.5 <- BIC(ols.5)
bic.ols.all <- BIC(ols.all)
bics <- cbind (bic.ols.null,bic.ols.1,bic.ols.2,bic.ols.3,bic.ols.4,bic.ols.5,bic.ols.all)
colnames(bics) <- c("OLS NULL","OLS 1","OLS 2","OLS 3","OLS 4","OLS 5","OLS ALL")
knitr::kable(t(bics), "simple")
summary(ols.2)
summary(ols.all)
anova(ols.2,ols.all)
predict.ols.2 <- predict(ols.2, newx = x.test)
mean((predict.ols.2 - y.test)^2) #MSE
library(glmnet)
grid <- 10^seq(10, -2, length = 100) #griglia di valori
ridge.mod <- glmnet(x.train, y.train, alpha = 0, lambda = grid, thresh = 1e-12)
par(mfrow=c(1,2))
plot(ridge.mod, xvar = "lambda", lwd = 2, col=c(1:7), xlim = c(-5,12))
abline(h=0,col="black", lty=2)
legend("topright", legend=names(ols.all$coefficients[-c(1)]), fill = c(1:7), cex=0.8)
plot(ridge.mod, lwd = 2, col=c(1:7))
abline(h=0,col="black", lty=2)
set.seed(1)
#Fittiamo la regressione ridge sul training set
cv.out.ridge <- cv.glmnet(x.train, y.train, alpha = 0)
plot(cv.out.ridge)
bestlam.ridge <- cv.out.ridge$lambda.min
bestlam.ridge
ridge.pred <- predict(ridge.mod, s = bestlam.ridge, newx = x.test)
mean((ridge.pred - y.test)^2) #MSE test
tLL <- ridge.mod$nulldev - deviance(ridge.mod) #devianza residuale
k <- ridge.mod$df #gradi di libertà
n <- ridge.mod$nobs #numero di osservazioni estratte
BIC.ridge <- log(n)*k - tLL
best.bic.ridge <- which.min(BIC.ridge)
best.bic.ridge #Posizione miglior BIC
plot(BIC.ridge,cex=2)
points(best.bic.ridge,BIC.ridge[best.bic.ridge],pch=19,cex=2)
BIC.ridge[best.bic.ridge] #Valore miglior BIC
lasso.mod <- glmnet(x.train, y.train, alpha = 1, lambda = grid)
par(mfrow=c(1,2))
plot(lasso.mod, lwd = 2, col=c(1:7), xvar = "lambda", xlim = c(-5,5))
abline(h=0,col="black", lty=2)
legend("topright", legend=names(ols.all$coefficients[-c(1)]), fill = c(1:7), cex=0.8)
plot(lasso.mod, lwd = 2, col=c(1:7))
abline(h=0,col="black", lty=2)
set.seed(1)
cv.out.lasso <- cv.glmnet(x.train, y.train, alpha = 1)
plot(cv.out.lasso)
bestlam.lasso <- cv.out.lasso$lambda.min
bestlam.lasso
lasso.pred <- predict(lasso.mod, s = bestlam.lasso, newx = x.test)
mean((lasso.pred - y.test)^2) #MSE
tLL <- lasso.mod$nulldev - deviance(lasso.mod) #devianza residuale
k <- lasso.mod$df #gradi di libertà
n <- lasso.mod$nobs #numero di osservazioni estratte
BIC.lasso <- log(n)*k - tLL
best.bic.lasso <- which.min(BIC.lasso)
best.bic.lasso #Posizione miglior BIC
BIC.lasso[best.bic.lasso] #Valore miglior BIC
plot(BIC.lasso,cex=2)
points(best.bic.lasso,BIC.lasso[best.bic.lasso],pch=19,cex=2)
x <- model.matrix(lpsa~., prostate.fit)
y <- prostate.data %>%
select(lpsa) %>%
unlist() %>%
as.numeric()
out.ridge <- glmnet(x, y, alpha = 0, lambda = grid)
#Fittiamo la Ridge sul dataset completo
ridge.coef <- predict(out.ridge, type = "coefficients", s = bestlam.ridge)
#Mostriamo i soli coefficienti non nulli usando il lambda risultante dalla CV
ridge.coef[ridge.coef != 0]
out.lasso <- glmnet(x, y, alpha = 1, lambda = grid)
#Fittiamo la Lasso sul dataset completo
lasso.coef <- predict(out.lasso, type = "coefficients", s = bestlam.lasso)
#Mostriamo i soli coefficienti non nulli usando il lambda risultante dalla CV
lasso.coef[lasso.coef != 0]
